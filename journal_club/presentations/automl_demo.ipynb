{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. Stationarity\n",
    "2. Differencing\n",
    "3. Machine Learning\n",
    "    1. Linear Models\n",
    "    2. Deep Learning\n",
    "4. AutoML with AutoGluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from random import random\n",
    "from numpy import transpose, matrix, sin, mean\n",
    "from numpy.linalg import lstsq\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "import pandas as pd\n",
    "from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stationarity\n",
    "\n",
    "* $\\mu$ is constant\n",
    "* $\\sigma$ is constant\n",
    "* No seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 6), dpi=80)\n",
    "n = 100\n",
    "\n",
    "# mean is constant but variance varies\n",
    "y = [random()*x if x%2 else (-1)*random()*x for x in list(range(n))[::-1]]\n",
    "ax1.plot(y)\n",
    "ax1.plot([0]*n)\n",
    "\n",
    "# variance is constant but mean is increasing\n",
    "y = [2 + 5*x + 25 * random() if x%2 else 2 + 5*x - 25 * random() for x in range(n)]\n",
    "ax2.plot(y)\n",
    "ax2.plot([2 + 5*x for x in range(n)])\n",
    "\n",
    "y = [sin(x/4) for x in range(n)]\n",
    "ax3.plot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differencing\n",
    "\n",
    "Differencing is a technique to transform a non-stationary time series into a stationary one. It involves subtracting the current value of the series from the previous one, or from a lagged value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [2 + 5*x + 25 * random() if x%2 else 2 + 5*x - 25 * random() for x in range(n)]\n",
    "y_diff = [y[x+1] - y[x]  for x in range(n-1)]\n",
    "\n",
    "plt.plot(y)\n",
    "plt.plot(y_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive Model - $AR(p)$\n",
    "\n",
    "$y_i = \\phi_0 + \\phi_1 y_{i-1}+ \\phi_2 y_{i-2}+ \\cdots + \\phi_p y_{i-p} + \\epsilon_i$\n",
    "\n",
    "$\\iff$\n",
    "\n",
    "$\\epsilon_i = y_i - \\phi_0 - \\sum_{j=1}^{p}\\phi_jy_{i-j}$\n",
    "\n",
    "Our goal is to minimize\n",
    "\n",
    "$\\sum_{i=p+1}^{n}\\epsilon^2 = \\sum_{i=p+1}^{n} (y_i - \\phi_0 - \\sum_{j=1}^{p}\\phi_jy_{i-j})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy data\n",
    "n = 100\n",
    "y = [2 + 5*sin(x) + 2*random()  for x in range(n)]\n",
    "plt.plot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show p=1 (manual)\n",
    "p = 1\n",
    "X = transpose(matrix([[1]*(len(y) - p), y[:-1]]))\n",
    "Y = transpose(matrix(y[p:]))\n",
    "coef = lstsq(X, Y, rcond=None)[0]\n",
    "print(f\"Coefficients: {[coef[0,0], coef[1,0]]}\")\n",
    "y_hat = [coef[0, 0]] + [coef[0, 0] + coef[1, 0]*x for x in y[:-1]]\n",
    "plt.plot(y)\n",
    "plt.plot(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to pick to p: ACF and PACF plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6), dpi=80)\n",
    "plot_acf(y, ax=ax1, lags=50)\n",
    "plot_pacf(y, ax=ax2, lags=50)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show plot when p=3\n",
    "model = AutoReg(y, lags=3)\n",
    "model_fit = model.fit()\n",
    "print('Coefficients: %s' % model_fit.params)\n",
    "y_hat = model_fit.predict()\n",
    "plt.plot(y)\n",
    "plt.plot(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Average Model - $MA(q)$\n",
    "\n",
    "$y_i = \\theta_0 + \\epsilon_i + \\theta_1 \\epsilon_{i-1}+ \\theta_2 \\epsilon_{i-2}+ \\cdots + \\theta_q \\epsilon_{i-q}$\n",
    "\n",
    "_In the MA model, rather than using the earlier values of a variable for prediction, it uses earlier prediction error terms which result when regressing a series from its past values._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform regression\n",
    "X = transpose(matrix([[1]*len(y), list(range(n))]))\n",
    "Y = transpose(matrix(y))\n",
    "coef = lstsq(X, Y, rcond=None)[0]\n",
    "y_hat = [coef[0, 0] + coef[1, 0]*x for x in y]\n",
    "plt.plot(y)\n",
    "plt.plot(y_hat)\n",
    "\n",
    "# use the erros from regression to find the MA coefficients\n",
    "q = 1\n",
    "errors = [y[x] - y_hat[x] for x in range(n)]\n",
    "X = transpose(matrix([[1]*(len(errors) - q), errors[:-1]]))\n",
    "Y = transpose(matrix(y[q:]))\n",
    "coef = lstsq(X, Y, rcond=None)[0]\n",
    "print(f\"Coefficients: {[coef[0,0], coef[1,0]]}\")\n",
    "y_hat = [coef[0, 0] + coef[1, 0]*x for x in y]\n",
    "plt.plot(y)\n",
    "plt.plot(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ARMA(p, q)$\n",
    "\n",
    "*AR and MA can be used together to form a new model called the Autoregressive Moving Average (ARMA)*\n",
    "\n",
    "$y_i = \\phi_0 + \\phi_1 y_{i-1}+ \\phi_2 y_{i-2}+ \\cdots + \\phi_p y_{i-p} + \\theta_0 + \\theta_1 \\epsilon_{i-1}+ \\theta_2 \\epsilon_{i-2}+ \\cdots + \\theta_q \\epsilon_{i-q} + \\epsilon_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ARIMA(p, q, d)$\n",
    "\n",
    "$d$ *refers to the number of differencing transformations required by the time-series to attain stationarity.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Models\n",
    "\n",
    "* Recurrent Neural Networks (RNN)\n",
    "* Gated Recurrent Unit (GRU)\n",
    "* Long Short-Term Memory (LSTM)\n",
    "* Indepedently Recurrent Neural Networks (IndRNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML\n",
    "\n",
    "* Model Selection\n",
    "* Hyperparameter Optimization\n",
    "* Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoGluon\n",
    "\n",
    "Visit the quickstart guide [here](https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-quick-start.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep timeseries dataframe\n",
    "item_id = \"H1\"\n",
    "prediction_length = 24\n",
    "\n",
    "x = list(map(lambda x: x*3.6*10**12, range(n)))\n",
    "x_train = x[:-prediction_length]\n",
    "x_test = x[-prediction_length:]\n",
    "y_train = y[:-prediction_length]\n",
    "y_test = y[-prediction_length:]\n",
    "\n",
    "df_train = pd.DataFrame({\"timestamp\": x_train, \"target\": y_train, \"item_id\": [item_id]*(n-prediction_length)})\n",
    "train_data = TimeSeriesDataFrame.from_data_frame(\n",
    "    df_train,\n",
    "    id_column=\"item_id\",\n",
    "    timestamp_column=\"timestamp\"\n",
    ")\n",
    "df_test = pd.DataFrame({\"timestamp\": x_test, \"target\": y_test, \"item_id\": [item_id]*prediction_length})\n",
    "test_data = TimeSeriesDataFrame.from_data_frame(\n",
    "    df_test,\n",
    "    id_column=\"item_id\",\n",
    "    timestamp_column=\"timestamp\"\n",
    ")\n",
    "train_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "predictor = TimeSeriesPredictor(\n",
    "    prediction_length=prediction_length,\n",
    "    target=\"target\",\n",
    "    eval_metric=\"MSE\",\n",
    ")\n",
    "predictor.fit(\n",
    "    train_data,\n",
    "    presets=\"medium_quality\",\n",
    "    time_limit=1200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.leaderboard(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 3))\n",
    "predictions = predictor.predict(train_data)\n",
    "y_past = train_data.loc[item_id][\"target\"]\n",
    "y_pred = predictions.loc[item_id]\n",
    "y_test = test_data.loc[item_id][\"target\"]\n",
    "\n",
    "plt.plot(y_past, label=\"Past time series values\")\n",
    "plt.plot(y_pred[\"mean\"], label=\"Mean forecast\")\n",
    "plt.plot(y_test, label=\"Future time series values\")\n",
    "\n",
    "plt.fill_between(\n",
    "    y_pred.index, y_pred[\"0.1\"], y_pred[\"0.9\"], color=\"red\", alpha=0.1, label=f\"10%-90% confidence interval\"\n",
    ")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
